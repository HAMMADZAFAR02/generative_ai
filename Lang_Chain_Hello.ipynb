{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6tFJOx95bl8C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "se_6qrvmH2JD"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key=userdata.get('GEMINI_API_KEY')"
      ],
      "metadata": {
        "id": "3iF3UGFpc1tR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_google_genai as genai"
      ],
      "metadata": {
        "id": "U_8wfLi6eZU8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "CAmPICjXe0xg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=ChatGoogleGenerativeAI(\n",
        "  model=\"gemini-1.5-flash\",\n",
        "  api_key=api_key\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "sr_d4dQWfkc7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "wkW5OI6Dh4iS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\",\"carreer,\",\"skill\"],\n",
        "    template=\"Supppose you are AI engineer.Based on the user skills: {skill} you answers the following question {question} \"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Combine both using LLMChain\n",
        "chain = LLMChain(llm=model, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "gqcWQ7y5h7Iw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Welcome to AI career counsellor mentor. How can I assist you today?\")\n",
        "skill = input(\"Enter your skills: \")\n",
        "in_question = input(\"Enter your question about AI (or type 'exit' to quit): \")\n",
        "\n",
        "while in_question.lower() != \"exit\":\n",
        "    try:\n",
        "        response = chain.invoke({\"skill\": skill, \"question\": in_question})  # Pass both skill and question\n",
        "        print(\"Answer:\", response[\"text\"]) # Access text attribute. Different llms might return answer differently.\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing your request: {e}\")\n",
        "    in_question = input(\"Enter your next question about AI (or type 'exit' to quit): \")\n"
      ],
      "metadata": {
        "id": "vXSvE630i4dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43vm3-tyh_OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "biwWl3iGNGP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MyOGSowcJvXo"
      }
    }
  ]
}